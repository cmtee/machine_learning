{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Resampling & Model Selection\n",
    "\n",
    "The goal of this lab is to give you the tools to:\n",
    "\n",
    " - Perform bootstrapping\n",
    " - Use Leave One Out Cross-Validation (LOOCV)\n",
    " - Perform feature selection with:\n",
    "    - Best Subset Selection\n",
    "    - Forward Stepwise Selection\n",
    "    - Backward Stepwise Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Our well-trusted libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sci-kit learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression, Lasso\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "# Statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Iterator building blocks\n",
    "# example: combinations('ABCD', 2) --> AB AC AD BC BD CD\n",
    "from itertools import combinations\n",
    "\n",
    "# Many were concerned with warnings. The short answer is that FutureWarning (most common)\n",
    "# appears when a functionality is deprecated and will be replaced. Here's how to ignore them\n",
    "# even if you should find a way to resolve them in a production environment.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this lab, we will use an extremely simple dataset as this helps with the intuition. In essence, we will try to determine whether a person will default (i.e., fail to make their interest or principal payments on a debt) with their account balance, income, and a student indicator. More precisely:\n",
    "\n",
    " - `default` (str, binary): Whether the individual defaulted\n",
    " - `student` (str, binary): Whether the individual is a student\n",
    " - `balance` (float, continuous): The individual's account balance\n",
    " - `income` (float, continuous): The individual's annual income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  default student      balance        income\n",
       "1      No      No   729.526495  44361.625074\n",
       "2      No     Yes   817.180407  12106.134700\n",
       "3      No      No  1073.549164  31767.138947\n",
       "4      No      No   529.250605  35704.493935\n",
       "5      No      No   785.655883  38463.495879"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "default\n",
       "No     9667\n",
       "Yes     333\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default = pd.read_csv('default.csv',index_col=0)\n",
    "\n",
    "\n",
    "display(default.head())\n",
    "print(default.shape, '\\n')\n",
    "default.value_counts('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn the `default` and `student` into binary variables. We're only using best practices after all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   default  student      balance        income\n",
       "1        0        0   729.526495  44361.625074\n",
       "2        0        1   817.180407  12106.134700\n",
       "3        0        0  1073.549164  31767.138947\n",
       "4        0        0   529.250605  35704.493935\n",
       "5        0        0   785.655883  38463.495879"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_dict = {'Yes': 1,'No': 0}\n",
    "for col in ['default', 'student']:\n",
    "    default[col] = default[col].map(encoding_dict)\n",
    "\n",
    "default.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go into resampling methods, let's estimate the model with the original sample (i.e, `default`) to get a sense of the estimates of the coefficients as well as the standard errors associated with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   const      balance        income\n",
       "1    1.0   729.526495  44361.625074\n",
       "2    1.0   817.180407  12106.134700\n",
       "3    1.0  1073.549164  31767.138947\n",
       "4    1.0   529.250605  35704.493935\n",
       "5    1.0   785.655883  38463.495879"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "5    0\n",
       "Name: default, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078948\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                default   No. Observations:                10000\n",
      "Model:                          Logit   Df Residuals:                     9997\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Fri, 14 Feb 2025   Pseudo R-squ.:                  0.4594\n",
      "Time:                        00:54:19   Log-Likelihood:                -789.48\n",
      "converged:                       True   LL-Null:                       -1460.3\n",
      "Covariance Type:            nonrobust   LLR p-value:                4.541e-292\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -11.5405      0.435    -26.544      0.000     -12.393     -10.688\n",
      "balance        0.0056      0.000     24.835      0.000       0.005       0.006\n",
      "income      2.081e-05   4.99e-06      4.174      0.000     1.1e-05    3.06e-05\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.14 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "X = default[['balance', 'income']]\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "y = default['default']\n",
    "\n",
    "display(X.head(), y.head())\n",
    "\n",
    "results = sm.Logit(y, X).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are interested in the estimate and the standard error of each coefficient. We will now see how resampling methods perform while keeping this as our initial finding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping\n",
    "\n",
    "Bootstrapping is a method for estimating sampling distributions of an estimator by resampling with replacement form the original sample. It is a flexible way to quantify uncertainty associated with an estimator or a model. Bootstrapping is especially useful because it allows us to mimic the process of selecting many observations from the population.\n",
    "\n",
    "We use bootstrapping for estimating the distribution of a statistic (e.g. mean, variance) without using normality assumptions; in this lab, we are going to estimate hard to calculate standard errors (or confidence intervals). It is also used when the population is too small to generate a large sample.\n",
    "\n",
    "The intuition behind the bootstrap is that we approximate the real distribution in the population by the original sample. This of course relies on the distribution in the original sample to approximate the population (which increases with n). Keep in mind: “Bootstrap confidence intervals are neither exact nor optimal, but aim instead for a wide applicability, combined with near-exact accuracy” (Efron and Hastie).\n",
    "\n",
    "Throughout, we will use the following terminology when referring to the samples:\n",
    "\n",
    "- **Original Sample:** The one sample of size `n` we have from the population\n",
    "- **Subsample:** We draw `B` subsamples (of size `n`) __with replacement__ from the original sample. Choosing how many bootstrap replications to perform is subjective; infinity is ideal, but we will settle for “as many as possible”.\n",
    "\n",
    "The bootstrap algorithm goes as follows:\n",
    "\n",
    "1. Obtain the original sample from the population (size `n`)\n",
    "2. Draw a subsample (with replacerment) from the original sample (size `n`)\n",
    "3. Compute the estimate ($ \\hat{\\alpha}_{b}^{*} $) from the subsample\n",
    "4. Repeat steps 2 and 3 `B` times\n",
    "5. Compute $ SE_{B}(\\hat{\\alpha}) = \\sqrt{\\frac{1}{B-1} \\sum_{b=1}^{B} (\\hat{\\alpha}_{b}^{*} - \\bar{\\alpha}_{b}^{*})^{2}} $ where $ \\bar{\\alpha}_{b}^{*} = \\frac{\\sum_{b=1}^{B} \\hat{\\alpha}_{b}^{*}}{B}$\n",
    "\n",
    "![Bootstrap Algorithm](bootstrap_algorithm.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyways, let's get into the code to replicate the bootstrap algorithm we just looked at. \n",
    "\n",
    "Step 1 is already done as `default` is our original sample of length `n = 10000`. \n",
    "\n",
    "Step 2 is to draw a subsample of `n` observations (with replacement) from the original sample. To do this, we will create the function `get_indices` to randomly select `n` indicies with replacement from our df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(data, n, seed):\n",
    "    '''\n",
    "    Generates a random subsample (i.e., its indices)\n",
    "    with replacement consisting of n observations each.\n",
    "    Inputs:\n",
    "        - data (pd.Dataframe): data to sample from\n",
    "        - n (int): number of observations in the sample\n",
    "        - seed (int): seed to set in the random number grenerator\n",
    "    Output:\n",
    "        - indices (np.ndarray): array of indices forming\n",
    "         the samples\n",
    "    '''\n",
    "    rng = np.random.default_rng(seed)    # allows you to set your seed\n",
    "    indices = rng.choice(data.index,     # Use the dataset's indices as the input\n",
    "                         int(n),         # Number of indices per sample\n",
    "                         replace=True    # Draw samples with replacement\n",
    "                        )\n",
    "    return indices\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to set your random seed; this ensures the data is randomized the same way everytime you call the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 360, 6940, 4224, 6415, 2636, 1287, 6060, 1138, 1202, 6534])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of making a call to the get_indices function\n",
    "get_indices(default, 10, 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, there is replacement, meaning that an observation can occur 0, 1, or multiple times in the subsample. Here's proof that it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9986,     1],\n",
       "       [ 9987,     1],\n",
       "       [ 9989,     3],\n",
       "       [ 9991,     2],\n",
       "       [ 9993,     1],\n",
       "       [ 9995,     2],\n",
       "       [ 9996,     2],\n",
       "       [ 9997,     2],\n",
       "       [ 9998,     1],\n",
       "       [10000,     1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(np.unique(get_indices(default, 10000, 45), return_counts=True)).T[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 is to estimate the coefficients with each subsample. We create a function `boot_fn` which performs the `logit` on each subsample in order to get the estimates and their standard errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to boot.fn in the exercise\n",
    "def boot_fn(data, index, constant=True, features=['balance','income'], target='default'):\n",
    "    '''\n",
    "    Runs a logistic regression (with a constant) on only the specified\n",
    "    indices within the data (i.e., on a single subsample).\n",
    "\n",
    "    Inputs:\n",
    "        - data (pd.Dataframe): data to sample from\n",
    "        - indices (np.ndarray): array of indices forming the samples\n",
    "        - features (lst of str): the name of the features\n",
    "        - target (str): the name of the target\n",
    "\n",
    "    Output:\n",
    "        - coefficients (lst of float): the coefficients\n",
    "    '''\n",
    "    X = data[features].loc[index]\n",
    "    if constant:\n",
    "        X = sm.add_constant(X)\n",
    "    y = data[target].loc[index]\n",
    "    \n",
    "    lr = sm.Logit(y,X).fit(disp=0)\n",
    "    coefficients = [lr.params[0], lr.params[1], lr.params[2]]\n",
    "\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients of a single subsample:\n",
      "\tIntercept:\t-10.84\n",
      "\tBalance:\t0.01\n",
      "\tIncome:\t\t0.0\n"
     ]
    }
   ],
   "source": [
    "# Example of making a call to the boot_fn function\n",
    "intercept, coef_balance, coef_income = boot_fn(default, get_indices(default, 10000, 45))\n",
    "print(f'Coefficients of a single subsample:\\n\\tIntercept:\\t{round(intercept, 2)}\\n\\tBalance:\\t{round(coef_balance, 2)}\\n\\tIncome:\\t\\t{round(coef_income, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step 4, we want to repeat steps 2 and 3 `B` number of times to create many subsamples and estimate their coefficients.  \n",
    "In step 5, we want to compute the mean and standard deviation based on an average of all the estimates.   \n",
    "We will do both in the `boot` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How long will this take to run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.005131721496582\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "#I want to take a 5 second nap\n",
    "time.sleep(5)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot(data, func, B):\n",
    "    '''\n",
    "    Executing a bootstrap over B subsamples\n",
    "    to estimate the (mean of) coefficients\n",
    "    and their associated standard errors.\n",
    "\n",
    "    Inputs:\n",
    "        - data (pd.Dataframe): data to sample from\n",
    "        - func (fn): function executing the regression\n",
    "        - B (int): number of subsamples\n",
    "    \n",
    "    Ouput:\n",
    "        - restults (dict of dicts): bootstrapped coefficients\n",
    "            and the associated standard errors\n",
    "    '''\n",
    "    # Step 4\n",
    "    coef_intercept = []\n",
    "    coef_balance = []\n",
    "    coef_income = []\n",
    "\n",
    "    coefs = ['intercept', 'balance', 'income']\n",
    "    output = {coef: [] for coef in coefs}\n",
    "    for i in range(B):\n",
    "        # set new seed (=i) every time you get a new subsample\n",
    "        reg_out = func(data, get_indices(data, len(data), i))\n",
    "        for i, coef in enumerate(coefs):\n",
    "            output[coef].append(reg_out[i])\n",
    "\n",
    "    # Step 5\n",
    "    results = {}\n",
    "    for coef in coefs:\n",
    "        results[coef] = {\n",
    "            'estimate': np.mean(output[coef]),\n",
    "            'std_err': np.std(output[coef])\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, the three functions we created are the building blocks to performing the bootstrap algorithm. \n",
    "\n",
    "Now, we can finally run our own bootstrap on the data we have! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept:\n",
      "\tEstimate: -11.599221788148167\n",
      "\tStandard Error: 0.44076600858304354\n",
      "Balance:\n",
      "\tEstimate: 0.005675350638457242\n",
      "\tStandard Error: 0.00023101506115923834\n",
      "Income:\n",
      "\tEstimate: 2.1092285173232854e-05\n",
      "\tStandard Error: 4.836558867887689e-06\n",
      "8.662756204605103\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "results = boot(default, boot_fn, 1000)\n",
    "for i, x in results.items():\n",
    "    print(f\"{i.capitalize()}:\\n\\tEstimate: {x['estimate']}\\n\\tStandard Error: {x['std_err']}\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get values that are very similar to what we expected (i.e., from the logistic regression on the original sample). However, by using Bootstrap, we reduce the risk of overfitting and improve the stability of our machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Side Bar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_indices in module __main__:\n",
      "\n",
      "get_indices(data, n, seed)\n",
      "    Generates a random subsample (i.e., its indices)\n",
      "    with replacement consisting of n observations each.\n",
      "    Inputs:\n",
      "        - data (pd.Dataframe): data to sample from\n",
      "        - n (int): number of observations in the sample\n",
      "        - seed (int): seed to set in the random number grenerator\n",
      "    Output:\n",
      "        - indices (np.ndarray): array of indices forming\n",
      "         the samples\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the help() function will return the docstring of your function; \n",
    "# you can remind yourself what a function does at any point throughout your code\n",
    "help(get_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave One Out Cross-Validation\n",
    "\n",
    "\n",
    "## Note that we could also use log loss instead of the Brier score (which in a binary outcome scenario is equivalent to MSE). \n",
    "\n",
    "We already went over cross-validation in previous labs. However, we have yet to cover its most extreme version; Leave One Out Cross Validation (LOOCV).\n",
    "\n",
    "As its name suggests, LOOCV is simply cross validation where the test set contains a single observation. There are a few pros and cons as to why you would choose to use cross-validation:\n",
    "\n",
    "- **Pros:**\n",
    "    - Low Upward Bias: because you only lose one observation when fitting the model on the training data\n",
    "    - Final Test Error Not Variable: The test error will be the same every time you run LOOCV on the dataset because it goes through the same validation process every time. \n",
    "- **Cons:**\n",
    "    - Computationally Costly: Requires estimating the model `n` different times\n",
    "    - Prone to higher variance (especially when n is small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iternation Milestones:...\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "\n",
      "[0.0005927380823778704, 0.00044313658761178823, 0.004082543662994585, 3.6509339069982594e-05, 0.0008426104370810332, 0.0010518241573572432, 0.0007861782470870673, 0.000504295884156107, 0.006093171030518981, 0.006208461720247789]\n",
      "\n",
      "MSE using LOOCV: 0.02824\n"
     ]
    }
   ],
   "source": [
    "squared_errors = []\n",
    "\n",
    "#How will the random seed affect LOOCV?\n",
    "# np.random.default_rng(5)\n",
    "np.random.default_rng(1)\n",
    "# np.random.default_rng(55)\n",
    "\n",
    "# Iterate over the entire dataset one observation at a time\n",
    "print('Iternation Milestones:...')\n",
    "for i in default.index:\n",
    "    # All except observation i is our training set\n",
    "    train = default.iloc[default.index != i,:]\n",
    "    # Only observation i is our test set\n",
    "    test = default.iloc[default.index == i,:]\n",
    "\n",
    "    # Fit the model and gather the squared error\n",
    "    ols = LinearRegression().fit(train[['balance', 'income']], train['default'])\n",
    "    test_predicted = ols.predict(test[['balance', 'income']])\n",
    "    test_actual = test[['default']]\n",
    "    squared_error = np.power(test_predicted - test_actual, 2)\n",
    "\n",
    "    # Store the squared error\n",
    "    squared_errors.append(squared_error.values[0][0])\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "\n",
    "print(f'\\n{squared_errors[:10]}\\n')\n",
    "\n",
    "# From the squared errors, get the Mean Squared Error (MSE)\n",
    "print(f'MSE using LOOCV: {round(np.mean(squared_errors), 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how certain errors are way larger than others? Those are outliers, but they average out over the size of dataset so we don't need to give them much attention.\n",
    "\n",
    "We can compare our results from LOOCV to regular KFold CV. An alternative way of doing k-fold cross validation is with the `cross_val_score` that is also in the sci-kit learn library. Here is how to use it with shuffling/randomizing of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02271776 -0.03038288 -0.02430095 -0.03140294 -0.0325379 ]\n",
      "0.028268485556214168\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = default[['balance','income']]\n",
    "Y = default['default']\n",
    "ols = LinearRegression()\n",
    "\n",
    "cv = KFold(5,              # Number of folds\n",
    "           shuffle=True,   # Randomizes observations into folds if true\n",
    "           random_state=9  # Affects how randomization occurs\n",
    "          )  \n",
    "\n",
    "cv_scores = cross_val_score(ols,      # Specified model\n",
    "                            X,        # Features\n",
    "                            Y,        # Target\n",
    "                            cv = cv,  # Number of folds\n",
    "                            scoring = 'neg_mean_squared_error'  # Metric\n",
    "                           )\n",
    "\n",
    "print(cv_scores)\n",
    "\n",
    "print(np.mean(-cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we tried to perform LOOCV on a smaller dataset and compare to the performance of KFold CV? \n",
    "\n",
    "To simulate this, we will randomly select smaller dataframes and perform LOOCV to compare how LOOCV changes as n increases. To do this, we can reuse our get_indices function from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE using LOOCV using n = 1000: 0.02797\n",
      "MSE using KFold CV using n = 1000: 0.02787\n",
      "MSE using LOOCV using n = 2500: 0.02738\n",
      "MSE using KFold CV using n = 2500: 0.02737\n",
      "MSE using LOOCV using n = 5000: 0.02875\n",
      "MSE using KFold CV using n = 5000: 0.02874\n",
      "MSE using LOOCV using n = 7500: 0.0282\n",
      "MSE using KFold CV using n = 7500: 0.02823\n",
      "MSE using LOOCV using n = 10000: 0.02846\n",
      "MSE using KFold CV using n = 10000: 0.02846\n"
     ]
    }
   ],
   "source": [
    "#Compare the test errors from KFoldCV to LOOCV over different n size datasets\n",
    "for n in [1000, 2500, 5000, 7500, 10000]:\n",
    "    # Subset datasets of different sized n\n",
    "    n_indices = get_indices(default, n, 42)\n",
    "    subset = default.loc[n_indices].reset_index(drop=True)\n",
    "\n",
    "    # Use the same code to get the LOOCV\n",
    "    squared_errors = []\n",
    "    for i in subset.index:\n",
    "        # All except observation i is our training set\n",
    "        train = subset.iloc[subset.index != i,:]\n",
    "        # Only observation i is our test set\n",
    "        test = subset.iloc[subset.index == i,:]\n",
    "\n",
    "        # Fit the model and gather the squared error\n",
    "        ols = LinearRegression().fit(train[['balance', 'income']], train['default'])\n",
    "        test_predicted = ols.predict(test[['balance', 'income']])\n",
    "        test_actual = test[['default']]\n",
    "        squared_error = np.power(test_predicted - test_actual, 2)\n",
    "\n",
    "        # Store the squared error\n",
    "        squared_errors.append(squared_error.values[0][0])  \n",
    "    print('MSE using LOOCV using n = '+ str(n) + f': {round(np.mean(squared_errors), 5)}')\n",
    "\n",
    "\n",
    "    # Use the same code to get the error from the KFold CV\n",
    "    X = subset[['balance','income']]\n",
    "    Y = subset['default']\n",
    "    ols = LinearRegression()\n",
    "\n",
    "    cv = KFold(5, shuffle=True, random_state=9)  \n",
    "\n",
    "    cv_scores = cross_val_score(ols, X, Y, cv = cv, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "    print('MSE using KFold CV using n = '+ str(n) + f': {round(np.mean(-cv_scores), 5)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our mini experiment, we see that the KFold CV performs almost equally to LOOCV when applied to the `default` dataset. In general, we prefer LOOCV in instances where n is small because it allows our model to fit over more training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Subset Selection\n",
    "\n",
    "\n",
    "## mlextend's Exhaustive Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 7/7"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: Features = ('balance', 'student'), Adjusted R² = 0.12292\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "\n",
    "X = default[['balance', 'income', 'student']]\n",
    "y = default['default']\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "efs = EFS(\n",
    "    model,\n",
    "    min_features=1,\n",
    "    max_features=3,  \n",
    "    scoring='r2',  \n",
    "    cv=5,  \n",
    "    n_jobs=-1 \n",
    ")\n",
    "\n",
    "efs.fit(X, y)\n",
    "\n",
    "best_subset = max(efs.subsets_.values(), key=lambda x: x['avg_score']) \n",
    "best_features = best_subset['feature_names']  \n",
    "best_adj_r2 = best_subset['avg_score']  \n",
    "\n",
    "print(f\"Best Model: Features = {best_features}, Adjusted R² = {round(best_adj_r2, 5)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the combination of features that best predict whether an individual will default is `student` and `balance`. This hints that `income` is an irrelevant variable when the others are given. This suggests that `income` is highly correlated with at least one of the other variables and thus only adds noise when predicting on the test set.\n",
    "\n",
    "Anyways, now we know which features form the best subset. However, computing all possible combinations is not always feasible. As seen in class, the more features, the larger the data, and the more computationally intensive the model, the longer it takes to compute all possible combinations. Therefore, there is a need for other feature selections methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn's Sequential Feature Selector, First forwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "## Using sklearn's Sequential Feature Selector:  General Setup\n",
    "\n",
    "default['CONSTANT'] = 1 \n",
    "\n",
    "X = default[['balance', 'income', 'student']]\n",
    "y = default['default']\n",
    "\n",
    "\n",
    "ols = LinearRegression()\n",
    "\n",
    "rs = 314\n",
    "\n",
    "kfcv = KFold(5,               \n",
    "             shuffle=True,    \n",
    "             random_state=rs  \n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Stepwise Selection\n",
    "\n",
    "The idea of this model is that we want to add the variable that provides the biggest improvement to our model (the one that yields the smallest error). We start with a model that has a constant as its only feature and continue until we run out of variables or see an increase in our error rate.\n",
    "\n",
    "Its algorithm is fairly intuitive:\n",
    "\n",
    "1. For k = 0, let $ M_{0} $ denote the null model (no Xs)\n",
    "2. For k = 1, ..., p:\n",
    "    - Consider all p - k + 1 models that add one predictor to $ M_{k-1} $\n",
    "    - Pick the best (smallest RSS or largest $ R^{2} $) of these models and call it $ M_{k} $\n",
    "3. Select the single best (CV test error, $ C_{p} $, AIC, BIC, or adjusted $ R^{2} $) model among $ M_{0}$, ..., $ M_{p}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Forward Stepwise Model by RMSE:\n",
      "['balance']\n",
      "\n",
      "Best Model RMSE: 0.02827\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "rmse_scorer = make_scorer(mean_squared_error, greater_is_better=False, squared=True)  # RMSE scoring\n",
    "\n",
    "sfs_forward = SequentialFeatureSelector(\n",
    "    ols,\n",
    "    n_features_to_select=\"auto\",\n",
    "    direction=\"forward\",\n",
    "    scoring=rmse_scorer,  # Use RMSE instead of R²\n",
    "    cv=kfcv\n",
    ")\n",
    "\n",
    "# Fit the feature selector\n",
    "sfs_forward.fit(X, y)\n",
    "\n",
    "# Extract selected features\n",
    "selected_features = X.columns[sfs_forward.get_support()].tolist()\n",
    "\n",
    "print(\"\\nBest Forward Stepwise Model by RMSE:\")\n",
    "print(selected_features)\n",
    "\n",
    "best_model_rmse = -cross_val_score(ols, X[selected_features], y, cv=kfcv, scoring=rmse_scorer).mean()\n",
    "print(f\"\\nBest Model RMSE: {round(best_model_rmse, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that forward stepwise selection chooses the model with `balance`. This disagrees with the \"Best\" subset selection.  Note there is no guarantee that we do find the best model from all possible combinations with forward stepwise selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Stepwise Selection\n",
    "The idea of this model is that we want to remove the variable that is not adding predictive power to our model (one that we can remove without increasing our error). We start with a full model (using all of our variables) and drop variables until we see an increase in our error rate or end up with only a constant. The idea remains the exact same as in forward stepwise selection, but we start at a different point.\n",
    "\n",
    "The algorithm goes as follows:\n",
    "\n",
    "1. For k = p, let $ M_{p} $ denote the fully specified model (all Xs)\n",
    "2. For k = p-1, ..., 0:\n",
    "    - Consider all p - k + 1 models that remove one predictor from $ M_{k+1} $\n",
    "    - Pick the best (smallest RSS or largest $ R^{2} $) of these models and call it $ M_{k} $\n",
    "3. Select the single best (CV test error, $ C_{p} $, AIC, BIC, or adjusted $ R^{2} $) model among $ M_{0}$, ..., $ M_{p}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Backward Stepwise Model by RMSE:\n",
      "['balance', 'student']\n",
      "\n",
      "Best Model RMSE: 0.02824\n"
     ]
    }
   ],
   "source": [
    "sfs_backward = SequentialFeatureSelector(\n",
    "    ols,\n",
    "    n_features_to_select=\"auto\",  # Automatically selects the best subset\n",
    "    direction=\"backward\",\n",
    "    scoring=rmse_scorer,\n",
    "    cv=kfcv\n",
    ")\n",
    "\n",
    "# Fit the feature selector\n",
    "sfs_backward.fit(X, y)\n",
    "\n",
    "# Extract selected features\n",
    "selected_features = X.columns[sfs_backward.get_support()].tolist()\n",
    "\n",
    "print(\"\\nBest Backward Stepwise Model by RMSE:\")\n",
    "print(selected_features)\n",
    "\n",
    "# Evaluate RMSE of the best model\n",
    "best_model_rmse = -cross_val_score(ols, X[selected_features], y, cv=kfcv, scoring=rmse_scorer).mean()\n",
    "print(f\"\\nBest Model RMSE: {round(best_model_rmse, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preserve for posterity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comb_betweenX(X):\n",
    "    '''\n",
    "    Returns a list with all possible combinations\n",
    "    of a given list of features.\n",
    "\n",
    "    Input:\n",
    "        - X (lst of str): the features\n",
    "\n",
    "    Output:\n",
    "        - comb (lst of lst): All possible\n",
    "          combinations\n",
    "    '''\n",
    "    interm = []\n",
    "    for i in range(1, len(X)+1):\n",
    "        interm.extend(list(combinations(X, i)))\n",
    "\n",
    "    comb = []\n",
    "    for i in interm:\n",
    "        comb.append(list(i))\n",
    "\n",
    "    return comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['student'], ['balance'], ['income'], ['student', 'balance'], ['student', 'income'], ['balance', 'income'], ['student', 'balance', 'income']]\n"
     ]
    }
   ],
   "source": [
    "comb = create_comb_betweenX(['student', 'balance', 'income'])\n",
    "print(comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have a list of all possible combinations that contain between 1 and `p` features. (`p = len(X.columns)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"['student']\": 0.03217450950283521,\n",
       " \"['balance']\": 0.028303358251519306,\n",
       " \"['income']\": 0.03219876684675046,\n",
       " \"['student', 'balance']\": 0.028263423110388763,\n",
       " \"['student', 'income']\": 0.03217256077477485,\n",
       " \"['balance', 'income']\": 0.028268485556214168,\n",
       " \"['student', 'balance', 'income']\": 0.028261871785201197}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['student', 'balance', 'income']\n"
     ]
    }
   ],
   "source": [
    "#Let's run one regression for every possible combination and store its value\n",
    "comb_dict = {}\n",
    "\n",
    "for i in comb: # For every combination\n",
    "    X = default[i]\n",
    "    Y = default['default']\n",
    "    ols = LinearRegression()\n",
    "    mse = np.mean(-1*cross_val_score(ols, \n",
    "                                     X, \n",
    "                                     Y, \n",
    "                                     cv = KFold(5, shuffle=True, random_state=9), \n",
    "                                     scoring = 'neg_mean_squared_error'))\n",
    "    comb_dict[str(i)] = mse\n",
    "\n",
    "display(comb_dict)\n",
    "print(min(comb_dict, key=comb_dict.get))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the combination of features that best predict whether an individual will default is `student` and `balance`. This hints that `income` is an irrelevant variable when the others are given. This suggests that `income` is highly correlated with at least one of the other variables and thus only adds noise when predicting on the test set.\n",
    "\n",
    "Anyways, now we know which features form the best subset. However, computing all possible combinations is not always feasible. As seen in class, the more features, the larger the data, and the more computationally intensive the model, the longer it takes to compute all possible combinations. Therefore, there is a need for other feature selections methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Initial run with only one var (constant term/only bias weight): ['constant']\n",
      "      Benchmark error: 0.03220745625\n",
      "\n",
      "\u001b[1mIteration: 0 \u001b[0m\n",
      " Running model with: ['constant', 'student']\n",
      "      Error: 0.03217450950283521\n",
      " Running model with: ['constant', 'balance']\n",
      "      Error: 0.0283033582515193\n",
      " Running model with: ['constant', 'income']\n",
      "      Error: 0.03219876684675046\n",
      "          *** Variable selected: balance\n",
      "          *** Min error selected: 0.0283033582515193\n",
      "\n",
      "\u001b[1mIteration: 1 \u001b[0m\n",
      " Running model with: ['constant', 'balance', 'student']\n",
      "      Error: 0.028263423110388756\n",
      " Running model with: ['constant', 'balance', 'income']\n",
      "      Error: 0.02826848555621416\n",
      "          *** Variable selected: student\n",
      "          *** Min error selected: 0.028263423110388756\n",
      "\n",
      "\u001b[1mIteration: 2 \u001b[0m\n",
      " Running model with: ['constant', 'balance', 'student', 'income']\n",
      "      Error: 0.02826187178520121\n",
      "          *** Variable selected: income\n",
      "          *** Min error selected: 0.02826187178520121\n",
      "\n",
      "\n",
      "Variables chosen for our model ['constant', 'balance', 'student', 'income']\n"
     ]
    }
   ],
   "source": [
    "# Add constant to dataframe\n",
    "default['constant'] = 1 \n",
    "\n",
    "# Specify target\n",
    "Y = default['default']\n",
    "\n",
    "# Variables to use in forward propagation\n",
    "vars_left_add = ['student', 'balance', 'income'] \n",
    "\n",
    "# Regression type\n",
    "ols = LinearRegression()\n",
    "\n",
    "# Starting variables (constant initially)\n",
    "current_vars = ['constant']\n",
    "\n",
    "X = default[current_vars]\n",
    "benchmark_error = np.mean(-1*cross_val_score(ols, X, Y,\n",
    "                                             cv = KFold(5, shuffle=True, random_state=9),\n",
    "                                             scoring = 'neg_mean_squared_error'))\n",
    "print(' Initial run with only one var (constant term/only bias weight):', current_vars)\n",
    "print('      Benchmark error:', benchmark_error)\n",
    "print('')\n",
    "\n",
    "# Keep adding the best variables (until no improvement can be made)\n",
    "for iter in range(len(vars_left_add)):\n",
    "    print('\\033[1m'+ 'Iteration:', iter, '\\033[0m')\n",
    "    error_list = []\n",
    "    # Iterate over all the variables left to add\n",
    "    for var in vars_left_add:\n",
    "        # Modify X according to current iteration\n",
    "        X = default[current_vars + [var]]\n",
    "        # Perform 5-fold CV to get errors\n",
    "        error = np.mean(-1*cross_val_score(ols, X, Y,\n",
    "                                           cv = KFold(5, shuffle=True, random_state=9),\n",
    "                                           scoring = 'neg_mean_squared_error'))\n",
    "        error_list.append(error)\n",
    "        print(' Running model with:', current_vars + [var])\n",
    "        print('      Error:', error)\n",
    "\n",
    "    # Chose the smallest error\n",
    "    min_error = min(error_list)\n",
    "    chosen_col_index = error_list.index(min_error)\n",
    "\n",
    "    # If our current smallest error is smaller than our previous error, than we add a variable\n",
    "    if min_error < benchmark_error:\n",
    "        print('          *** Variable selected:', vars_left_add[chosen_col_index])\n",
    "        print('          *** Min error selected:', min_error)\n",
    "        print('')\n",
    "        # Add the variable that produced the smallest error to current_vars\n",
    "        current_vars.append(vars_left_add[chosen_col_index])\n",
    "        # Delete chosen variable from vars_left_add\n",
    "        del vars_left_add[chosen_col_index] \n",
    "        # Update benchmark_error\n",
    "        benchmark_error = min_error\n",
    "    \n",
    "    # Otherwise, we stop our model\n",
    "    else:\n",
    "        print('          \\033[4m*** No variable was selected', '\\033[0m')\n",
    "        print('          *** Previous error rate (', benchmark_error,') is lower than smallest error rate of this iteration (', min_error ,')')\n",
    "        print('          *** Break')\n",
    "        break\n",
    "\n",
    "print('')\n",
    "print('Variables chosen for our model', current_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that forward stepwise selection chooses the model with a constant, `balance`, `student` and `income`. This coincides with the best subset selection we performed previously. However, there is no guarantee that we do find the best model from all possible combinations with forward stepwise selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backwards Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Initial run with all vars: ['constant', 'student', 'balance', 'income']\n",
      "      Benchmark error: 0.028261871785201197\n",
      "\n",
      "\u001b[1mIteration: 0 \u001b[0m\n",
      " Running model with: ['constant', 'balance', 'income']\n",
      "      Error: 0.02826848555621416\n",
      " Running model with: ['constant', 'student', 'income']\n",
      "      Error: 0.03217256077477485\n",
      " Running model with: ['constant', 'student', 'balance']\n",
      "      Error: 0.028263423110388746\n",
      "          \u001b[4m*** No variable was selected \u001b[0m\n",
      "          *** Previous error rate ( 0.028261871785201197 ) is lower than smallest error rate of this iteration ( 0.028263423110388746 )\n",
      "          *** Break\n",
      "\n",
      "Variables chosen for our model ['constant', 'student', 'balance', 'income']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# Add constant to dataframe\n",
    "default['constant'] = 1 \n",
    "\n",
    "# Specify the target\n",
    "Y = default['default']\n",
    "\n",
    "# Variables to remove iteratively\n",
    "vars_left_to_drop = ['student', 'balance', 'income'] \n",
    "\n",
    "# Regression type\n",
    "ols = LinearRegression()\n",
    "\n",
    "# Starting variables (all)\n",
    "current_vars = ['constant'] + vars_left_to_drop\n",
    "\n",
    "X = default[current_vars]\n",
    "benchmark_error = np.mean(-1*cross_val_score(ols, X, Y,\n",
    "                                             cv = KFold(5, shuffle=True, random_state=9),\n",
    "                                             scoring = 'neg_mean_squared_error'))\n",
    "print(' Initial run with all vars:', current_vars)\n",
    "print('      Benchmark error:', benchmark_error)\n",
    "print('')\n",
    "\n",
    "\n",
    "# Keep removing the worst variables (until no improvement can be made)\n",
    "for iter in range(len(vars_left_to_drop)):\n",
    "    print('\\033[1m'+ 'Iteration:', iter, '\\033[0m')\n",
    "    error_list = []\n",
    "    # Iterate over all the variables left to remove\n",
    "    for var in vars_left_to_drop:\n",
    "        # Modify X according to current iteration\n",
    "        vars_to_be_used = ['constant'] + [i for i in vars_left_to_drop if i != var]\n",
    "        X = default[['constant'] + [i for i in vars_left_to_drop if i != var]]\n",
    "        # Perform 5-fold CV to get errors\n",
    "        error = np.mean(-1*cross_val_score(ols, X, Y,\n",
    "                                           cv = KFold(5, shuffle=True, random_state=9),\n",
    "                                           scoring = 'neg_mean_squared_error'))\n",
    "        error_list.append(error)\n",
    "        print(' Running model with:', vars_to_be_used)\n",
    "        print('      Error:', error)\n",
    "\n",
    "    # Chose the largest error\n",
    "    min_error = min(error_list)\n",
    "    chosen_col_index = error_list.index(min_error)\n",
    "\n",
    "    # If our current smallest error is smaller than our previous error, then we drop the variable associated with it\n",
    "    if min_error < benchmark_error:\n",
    "        print('          *** Will drop:', vars_left_to_drop[chosen_col_index])\n",
    "        print('          *** Min error selected:', min_error)\n",
    "        print('          *** Chose the variable that generated the min error + was lower than previous error')\n",
    "        print('')\n",
    "        # Delete chosen variable from current_vars and vars_left_to_drop\n",
    "        del current_vars[chosen_col_index + 1]\n",
    "        del vars_left_to_drop[chosen_col_index]\n",
    "        # Update benchmark_error\n",
    "        benchmark_error = min_error\n",
    "    \n",
    "    # If not, we keep our model\n",
    "    else:\n",
    "        print('          \\033[4m*** No variable was selected', '\\033[0m')\n",
    "        print('          *** Previous error rate (', benchmark_error,') is lower than smallest error rate of this iteration (', min_error ,')')\n",
    "        print('          *** Break')\n",
    "        break\n",
    "\n",
    "print('')\n",
    "print('Variables chosen for our model', current_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
