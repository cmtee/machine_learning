{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"ML_PS2\"\n",
        "author: \"Clarice Tee\"\n",
        "date: February 6, 2025\n",
        "format: \n",
        "  pdf:\n",
        "    include-in-header: \n",
        "       text: |\n",
        "         \\usepackage{fvextra}\n",
        "         \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n",
        "include-before-body:\n",
        "  text: |\n",
        "    \\RecustomVerbatimEnvironment{verbatim}{Verbatim}{\n",
        "      showspaces = false,\n",
        "      showtabs = false,\n",
        "      breaksymbolleft={},\n",
        "      breaklines\n",
        "    }\n",
        "---\n",
        "\n",
        "\n",
        "# Part 1\n"
      ],
      "id": "aba71201"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.formula.api as smf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot\n",
        "import os\n",
        "import statsmodels.formula.api as smf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error"
      ],
      "id": "2f3f39b5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.a.\n",
        "QDA is expected to perform better on the training set/\n",
        "This is because the QDA's greater flexibility yields a closer fit, but does have greater variance.\n",
        "\n",
        "LDA is expected to do better on the test. While the QDA is flexible, this means it runs into the\n",
        "problem of overfitting the linear Bayes decision boundary.\n",
        "\n",
        "1.b.(Non-linear)\n",
        " While QDA's flexibility increases its variance compared to LDA, when the Bayes boundary is non-linear, this flexibility is actually aa good thing because it can be offset by a larger reduction in bias, thus better test performance.\n",
        "\n",
        "1.c.\n",
        "We expect the tesst predition accuracy of the QDA to improve relative to the LDA\n",
        "as the sample size  increases since is flexibility will yield a better fit, especially when we have more samples and this also helps deal with the problem of variance.\n",
        "We expect the test prediction accuracy of QDA relative to LDA to improve.\n",
        "\n",
        "1.d.\n",
        "False.Flexible methods like QDA require more data to prevent overfitting, which happens due to the model's sensitivity to the noise in the training sets. Overfitting\n",
        "would make the QDA have a higher test error rate than the LDA, which already approximates the Bayes decision boundary accurately. \n",
        "\n",
        "\n",
        "\n",
        "# Part 2\n",
        "\n",
        "2.a.\n",
        "X = [40 hours, 3.5 GPA]\n",
        "From the logistic regression model, we can fill in the formula:\n",
        "P(Y=1)|X = exp(-6 + 0.05 * X1 + X2) / (1 + exp(-6 + 0.05 * X1 + X2))"
      ],
      "id": "efe20ce4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X1 = 40  \n",
        "X2 = 3.5  \n",
        "\n",
        "\n",
        "probability_A = np.exp(-6 + 0.05 * X1 + X2) / (1 + np.exp(-6 + 0.05 * X1 + X2))\n",
        "\n",
        "print(f\"Probability of getting an A is {probability_A * 100:.1f}%\")"
      ],
      "id": "6ad597fd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.b.\n",
        "Same student, 50%, how many hours (X1)\n",
        "\n",
        "P(Y=.5)|X is .5 = exp(-6 + 0.05 * X1 + 3.5) / (1 + exp(-6 + 0.05 * X1 + 3.5))\n",
        "\n",
        "solving for x1 gives us:\n",
        "exp(0.05X1 âˆ’ 2.5) = 1"
      ],
      "id": "c1a0b24f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X1 = 2.5 / 0.05\n",
        "\n",
        "print(f\"Student who wants a  50% probability needs to study {X1} hours\")"
      ],
      "id": "5bd4e127",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 3\n",
        "\n",
        "Ave profit with dividends(X bar) =10\n",
        "Ave profit 1/0 dividends(X bar) =0\n",
        "\n",
        "Variance =36\n",
        "\n",
        "80% issued dividends.\n",
        "\n",
        "Using Bayes' theorem:\n",
        "\n",
        "A. Using normal distribution: Get likelihood for a company with profits X=4 to be in with dividend group\n",
        "\n",
        "B. Use 80% with dividend and 20% without dividend to wieigh likelihoods.\n",
        "\n",
        "C. Calculate posterior probability (weighted likelihood of with dividend group divided by the weighted likelihoods of with and without dividend)\n"
      ],
      "id": "9962f270"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "w_dividend = 0.8\n",
        "wo_dividend = 0.2\n",
        "exp_w_dividend = np.exp(-0.5)\n",
        "exp_wo_dividend = np.exp(-2 / 9)\n",
        "\n",
        "# Compute posterior probability\n",
        "posterior_w_dividend = (w_dividend * exp_w_dividend) / (w_dividend * exp_w_dividend + wo_dividend * exp_wo_dividend)\n",
        "\n",
        "print(f\"Probability of issuing a dividend: {posterior_w_dividend:.4f} or {posterior_w_dividend * 100:.2f}%\")"
      ],
      "id": "0de314ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 4\n"
      ],
      "id": "b01021cd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load the dataset\n",
        "directory = r\"C:\\Users\\clari\\OneDrive\\Documents\\Machine Learning\\ps2\"\n",
        "auto_path = os.path.join(directory, \"Data-Auto.csv\")\n",
        "auto_df = pd.read_csv(auto_path)\n",
        "print(auto_df.dtypes)\n",
        "print(auto_df.shape)"
      ],
      "id": "31692cf0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.a.\n",
        "Making dummy variable"
      ],
      "id": "50bfce2f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "auto_df[\"mpg01\"] = np.where(auto_df[\"mpg\"] > auto_df[\"mpg\"].median(), 1, 0)"
      ],
      "id": "40c34215",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.b. It looks like horsepower, acceleration, and weight could be useful in predicting mpg01 because the number of observations and the value of the variable tends to increase or decrease based on if mpg01 is 0 or 1 (although there are some overlaps)\n",
        "\n",
        "Scatterplot"
      ],
      "id": "9ddeed46"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# creating a list of variables to loop through\n",
        "auto_vars = ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin'] #excluding name, which is an object\n",
        "\n",
        "\n",
        "for var in auto_vars:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.scatterplot(x=var, y='mpg01', data=auto_df)\n",
        "    plt.title(f'Scatter Plot: {var} vs mpg')\n",
        "    plt.tight_layout\n",
        "    plt.show()"
      ],
      "id": "dc13133d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Boxplot"
      ],
      "id": "ddf88176"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for var in auto_vars:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(x='mpg01', y=var, data=auto_df)\n",
        "    plt.title(f'{var} Distribution by MPG Category', size=14)\n",
        "    plt.xlabel('High MPG (1) vs Low MPG (0)')\n",
        "    plt.ylabel(var)\n",
        "    plt.show()"
      ],
      "id": "50dcaee7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the scatterplots, we see a bit of a patterm from horsepower, acceleration,and weight, where mpg01 being equal to 0 or 1 is more likely based on whether these variables have higher or lower values. ALthough there is some overlap, it  at least shows greater distinctions compared to the other variables.\n",
        "\n",
        "From the boxplots, we can see that the median weight of mgp01 =1 cars much lower than that of mpg01 = 0 cars.So it may suggest heavier cars have lower mpg. We also see that mpg01=1 cars have much lower horsepower as mpg01 = 0 cars (though they have more variance in values)While acceleration shows less clear separation than weight/horsepower in the  plots, it still shows a trend where faster acceleration (lower values) tends to be associated with lower mpg (mpg01=0)\n",
        "\n",
        "Meanwhile, the other vairables don't show as clear patterns or have more overlaps in terms of the values of mpg01=0 or =1.\n",
        "\n",
        "4.c.Splitting to traiing and test set"
      ],
      "id": "ecec0183"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = auto_df[['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin']]\n",
        "\n",
        "y = auto_df[\"mpg01\"]\n",
        "\n",
        "# Train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=22)\n",
        "\n",
        "\n",
        "# Sanity check for index matching\n",
        "display(X_train.head(), X_test.head(), y_train.head(), y_test.head())"
      ],
      "id": "391a65cd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Make a copy of the training and test data\n",
        "X_train_dummy = X_train.copy()\n",
        "X_test_dummy = X_test.copy()\n",
        "\n",
        "## Insert the dummy variable in each set. \n",
        "## df.insert(column #, 'column name', value)\n",
        "X_train_dummy.insert(0, 'test', 0)\n",
        "X_test_dummy.insert(0, 'test', 1)\n",
        "\n",
        "\n",
        "X_full = pd.concat([X_test_dummy, X_train_dummy], axis = 0)\n",
        "\n",
        "display(X_full)\n",
        "print(X_full['test'].value_counts())"
      ],
      "id": "99c65bd8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run regression"
      ],
      "id": "379f4b41"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result = smf.ols(\n",
        "    'test ~ mpg + cylinders + displacement + horsepower + weight + acceleration + year + origin',\n",
        "    data=X_full\n",
        ").fit()\n",
        "print(result.summary())"
      ],
      "id": "b90dd4d9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since all  the p-values are larger than 0.1 we aren't as worried that the train and test set are significantly different, but we do want to check distribuiton of the training and test sets to see if they are balanced on characteristics we haven't included or are unobservable\n"
      ],
      "id": "579e7c8d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## Check the  distribution in the training and test sets\n",
        "\n",
        "print(f'Percentage of positive labels in the test set: {round(y_test.mean()*100, 2)}')\n",
        "print(f'Percentage of positive labels in the training set: {round(y_train.mean()*100, 2)}')"
      ],
      "id": "4848cd75",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.d. LDA"
      ],
      "id": "aab3c2d9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "# Choosing predictors related to mpg01\n",
        "X_train_rel = X_train.copy()[[\"horsepower\", \"weight\", \"acceleration\"]]\n",
        "X_test_rel = X_test.copy()[[\"horsepower\", \"weight\", \"acceleration\"]]\n",
        "\n",
        "# Fit the LDA model\n",
        "lda_model = LinearDiscriminantAnalysis()\n",
        "lda_model.fit(X_train_rel, y_train)\n",
        "# view the predicted test values\n",
        "y_pred_lda = lda_model.predict(X_test_rel)\n",
        "y_pred_lda"
      ],
      "id": "c63cbe87",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing error rate: #cite how to get this"
      ],
      "id": "39eabb63"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve\n",
        "\n",
        "error_rate_lda = 1 - accuracy_score(y_test, y_pred_lda)\n",
        "\n",
        "print(f\"The error rate is: {round(error_rate_lda, 4)*100}%\")"
      ],
      "id": "7c4c0786",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.e. QDA model\n"
      ],
      "id": "8ab1e53f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "qda_model = QuadraticDiscriminantAnalysis()\n",
        "qda_model.fit(X_train_rel, y_train)\n",
        "\n",
        "# view the predicted test values\n",
        "y_pred_qda = qda_model.predict(X_test_rel)\n",
        "y_pred_qda"
      ],
      "id": "cfc97744",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get  the error rate\n",
        "error_rate_qda = 1 - accuracy_score(y_test, y_pred_qda)\n",
        "\n",
        "print(f\"The QDA model's error rate is: {round(error_rate_qda, 4)*100}%\")"
      ],
      "id": "60a3c760",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.f. Logistic regression"
      ],
      "id": "e03f1a9d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fit the model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logisticRegr = LogisticRegression(max_iter=500)\n",
        "logisticRegr.fit(X_train_rel, y_train)\n",
        "\n",
        "# Predict the test set\n",
        "y_pred_log = logisticRegr.predict(X_test_rel)"
      ],
      "id": "71f3e1c8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get test error"
      ],
      "id": "51bb60ae"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "error_rate_log = 1 - accuracy_score(y_test, logisticRegr.predict(X_test_rel))\n",
        "\n",
        "# Print the error rate\n",
        "print(f\"The logistic regression model's error rate is: {round(error_rate_log, 4)*100}%\")"
      ],
      "id": "a44ed71c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.g."
      ],
      "id": "6b19c410"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "bayes_model = GaussianNB()\n",
        "bayes_model.fit(X_train_rel, y_train)\n",
        "# view the predicted test values\n",
        "y_pred_bayes = bayes_model.predict(X_test_rel)\n",
        "y_pred_bayes\n",
        "# Compute the error rate\n",
        "error_rate_bayes = 1 - accuracy_score(y_test, y_pred_bayes)\n",
        "# Print the error rate\n",
        "print(f\"The Naive Bayes model has an error rate of: {round(error_rate_bayes, 4)*100}%\")"
      ],
      "id": "a08645c2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://builtin.com/artificial-intelligence/gaussian-naive-bayes\n",
        "\n",
        "\n",
        "# Part 5\n",
        "5.a."
      ],
      "id": "0cb60193"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load the dataset\n",
        "directory = r\"C:\\Users\\clari\\OneDrive\\Documents\\Machine Learning\\ps2\"\n",
        "default_path = os.path.join(directory, \"Data-Default.csv\")\n",
        "default_df = pd.read_csv(default_path)\n",
        "print(default_df.dtypes)\n",
        "print(default_df.shape)\n",
        "default_df.head(5)"
      ],
      "id": "5d3b5dcd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Logistic Regression of income and balance on default\n"
      ],
      "id": "d98a2957"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cchange default into a dummy variable\n",
        "default_df[\"default01\"] = default_df[\"default\"].map({\"Yes\": 1, \"No\": 0})\n",
        "\n",
        "print(default_df[\"default01\"].value_counts(normalize=True) * 100)  \n",
        "# Checking if it worked\n",
        "yes_rows = default_df[default_df[\"default\"] == \"Yes\"]\n",
        "print(yes_rows.head(5))\n",
        "no_rows = default_df[default_df[\"default\"] == \"No\"]\n",
        "print(no_rows.head(5))"
      ],
      "id": "4997f15a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Defining X and y\n",
        "X_default = default_df[[\"income\", \"balance\"]]  \n",
        "y_default = default_df[\"default01\"]\n",
        "\n",
        "# Logisitc regression model\n",
        "default_logit_reg = LogisticRegression(max_iter=500)\n",
        "default_logit_reg.fit(X_default,y_default)"
      ],
      "id": "772b8284",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.b.Spit data, random seed 42, .7"
      ],
      "id": "f89fa302"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_train, X_validation, y_train, y_validation = train_test_split(X_default, y_default, train_size=0.7, random_state=42)\n",
        "# Sanity check\n",
        "display(X_train.head(), X_validation.head(), y_train.head(), y_validation.head())"
      ],
      "id": "00f666cc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fit the training data into logistic regression\n",
        "default_logit_train= LogisticRegression(max_iter=500)\n",
        "default_logit_train.fit(X_train,y_train)"
      ],
      "id": "171f5549",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Predict the validation set\n",
        "y_pred_log = default_logit_train.predict_proba(X_validation)[:, 1]\n",
        "print(\"Predicted probabilities above 0.5:\", y_pred_log[y_pred_log >= 0.5])\n",
        "print(\"Count of values >= 0.5:\", len(y_pred_log[y_pred_log >= 0.5]))"
      ],
      "id": "8e77a2b4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Classifying to default category if porbablity is > 0.5\n",
        "y_pred_log = np.array(y_pred_log)  \n",
        "y_pred_log = y_pred_log.astype(float)\n",
        "y_default_category = np.where(y_pred_log >= 0.5, 1, 0)\n",
        "print(y_pred_log[:10])  # First 10 predictions\n",
        "print(type(y_pred_log))  # Type check\n",
        "print(pd.Series(y_default_category).value_counts())"
      ],
      "id": "e6f8e809",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute the error rate\n",
        "error_valid = 1 - accuracy_score(y_validation, y_default_category)\n",
        "# Print the error\n",
        "print(f\"The validation set error is {round(error_valid, 4)*100}%\")"
      ],
      "id": "86e943c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.c."
      ],
      "id": "96225693"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = default_df[[\"income\", \"balance\"]]  \n",
        "y = default_df[\"default01\"]\n",
        "\n",
        "\n",
        "random_states = [2, 6, 9]\n",
        "error_rates = []\n",
        "\n",
        "for state in random_states:\n",
        "    # Split data with current random state\n",
        "    X_train, X_validation, y_train, y_validation = train_test_split(\n",
        "        X, y, \n",
        "        train_size=0.7, \n",
        "        random_state=state\n",
        "    )\n",
        "    \n",
        "    # Train model\n",
        "    default_logit_train = LogisticRegression(max_iter=500)\n",
        "    default_logit_train.fit(X_train, y_train)\n",
        "    \n",
        "    # Predict and calculate error\n",
        "    y_pred_log = default_logit_train.predict_proba(X_validation)[:, 1]\n",
        "    y_default_category = np.where(y_pred_log > 0.5, 1, 0)\n",
        "    error_rate = 1 - accuracy_score(y_validation, y_default_category)\n",
        "    \n",
        "    error_rates.append(error_rate)\n",
        "    print(f\"Random state {state}: validation error = {error_rate:.2%}\")\n",
        "\n",
        "# Analyze results\n",
        "print(\"\\nSummary:\")\n",
        "print(f\"Average error rate: {np.mean(error_rates):.2%}\")\n",
        "print(f\"Standard deviation: {np.std(error_rates):.2%}\")"
      ],
      "id": "646ef015",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Consistency: The error rates across the three relatively close, ranging from 2.37% to 3.07%, meaning it's not overly sensitive to how the data is split\n",
        "- Low Error Rates: They all have lower error rates than the random state 42 split.\n",
        "Slight Variability: There is some variability in the results, with a standard deviation of 0.31%. This variability is expected due to the random nature of the splits and demonstrates the importance of using multiple splits to assess model performance.\n",
        "\n",
        "These results give us some level of confidence in the model's performance and its ability to generalize to new data. Butmaybe doing the k-fold cross-validation will get us an even more robust estimate of the model's performance.\n",
        "\n",
        "5.d."
      ],
      "id": "f42e4389"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create dummy variable for student\n",
        "default_df[\"student01\"] = default_df[\"student\"].map({\"Yes\": 1, \"No\": 0})\n",
        "\n",
        "# Checking if it worked\n",
        "yes_rows = default_df[default_df[\"student\"] == \"Yes\"]\n",
        "print(yes_rows.head(5))\n",
        "no_rows = default_df[default_df[\"student\"] == \"No\"]\n",
        "print(no_rows.head(5))"
      ],
      "id": "5ffd40cc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Deine X, y\n",
        "X = default_df[[\"income\", \"balance\", \"student01\"]]  \n",
        "y = default_df[\"default01\"]\n",
        "\n",
        "# Logisitc regression model\n",
        "student_logit_reg = LogisticRegression(max_iter=500)\n",
        "student_logit_reg.fit(X,y)"
      ],
      "id": "0306ed6b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.7, random_state=42)\n",
        "# Sanity check\n",
        "display(X_train.head(), X_validation.head(), y_train.head(), y_validation.head())"
      ],
      "id": "8f80abd2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fit the training data into logistic regression\n",
        "student_logit_train= LogisticRegression(max_iter=500)\n",
        "student_logit_train.fit(X_train,y_train)"
      ],
      "id": "a1c00271",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Predict the validation set\n",
        "student_y_pred_log = student_logit_train.predict_proba(X_validation)[:, 1]\n",
        "student_y_pred_log[:5]"
      ],
      "id": "1da91850",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Classifying to default category if porbablity is > 0/5\n",
        "student_y_default_category = np.where(student_y_pred_log > 0.5, 1 , 0)"
      ],
      "id": "9b9d532d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute the error rate\n",
        "error_valid = 1 - accuracy_score(y_validation, student_y_default_category)\n",
        "# Print the error\n",
        "print(f\"The validation set error is {round(error_valid, 4)*100}%\")\n",
        "print(pd.Series(student_y_default_category).value_counts())"
      ],
      "id": "cff86115",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adding in student dummy variable didn't change the test error rate of the validation set.\n",
        "This can be interpreted as: being a student doesn't affect one's probability of default, \n",
        "all else equal. This doesn't match with our expectations because being a student probably\n",
        "affects default. Maybe if we added in the other variable like balance and income into the model,\n",
        "this may lower the error rate."
      ],
      "id": "e8d7b703"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\clari\\anaconda3\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}